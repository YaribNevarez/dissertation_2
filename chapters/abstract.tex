\chapter*{Abstract}
\thispagestyle{empty}
The use of \gls{ai} is entering a new era based on the use of ubiquitous embedded connected devices. The sustainability of this transformation requires the adoption of design techniques that reconcile accurate results with cost-effective system architectures. As such, improving the efficiency of \gls{ai} hardware engines as well as \gls{ml} portability must be considered.

In the emerging era of Industry 4.0, \gls{ml} algorithms yield the power of \gls{ai} to massively ubiquitous \gls{iot} devices. Applications in this field become smarter and more profitable as the availability of big data gets expanded, driving evolution of many aspects in science, industry, and daily life. However, state-of-the-art \gls{ml} algorithms, specially \glspl{snn} and \glspl{cnn}, represent elevated computational and energy cost. Therefore, hardware efficiency is one of the major goals to innovate compute engines as they are the machinery of the future.

Energy, performance, and chip-area are the key design concerns in computer systems. Considering the intrinsic error resilience of \gls{ml} algorithms, paradigms such as approximate computing come to the rescue by offering promising efficiency gains to assist the aforementioned design concerns. Approximation techniques are widely used in \gls{ml} algorithms at the model-structure as well as at the hardware processing level. However, state-of-the-art methods do not sufficiently address accelerator designs for \glspl{ann}, in particular with \gls{fp} computation.

To sustain the continuous expansion of \gls{ml} applications on cost-effective compute devices, approximate computing will gradually transform from a design alternative to an essential feature. This dissertation focuses on the investigation of design methodologies to exploit the intrinsic error resilience of \gls{ml} algorithms to optimize \gls{fp} inference in low-power embedded systems.

In the field of \glspl{snn}, this dissertation presents a hardware design methodology for low-power inference of \gls{sbs} neural networks targeting embedded applications. This \gls{ml} algorithm provides exceptional noise robustness and reduced complexity compared to conventional \gls{snn} with \gls{lif} mechanism. However, \gls{sbs} networks represent a memory footprint and a computational cost unsuitable for embedded applications. To address this problem, this work exploits the intrinsic error resilience of \gls{sbs} to improve performance and to reduce hardware complexity. More precisely, we design a vector dot-product module based on approximate computing with configurable quality using hybrid custom \gls{fp} and logarithmic number representations. This approach reduces computational run-time, memory footprint, and power dissipation while preserving inference accuracy. To demonstrate this approach, we address a design exploration flow with \gls{hls} on a \gls{fpga}. The proposed design reduces $20.5\times$ run-time and $8\times$ weight memory footprint, with less than $0.5\%$ of accuracy degradation without retraining on a handwritten digit classification task.

In the field of \glspl{cnn}, this dissertation presents a hardware design methodology for low-power inference targeting sensor analytics applications. In this work, we present the \gls{hf6} quantization and its dedicated hardware processor. We propose an optimized \gls{fp} \gls{mac} hardware by reducing the mantissa multiplication to a multiplexer-adder operation. We exploit the intrinsic error tolerance of neural networks to further reduce the hardware design with approximation on the subnormal number computation. To preserve model accuracy, we present a \gls{qat} method, which in some cases improves accuracy. We demonstrate this concept in 2D convolution layers. We present a lightweight \gls{tp} implementing a pipelined vector dot-product. For \gls{ml} portability, the custom \gls{fp} representation is wrapped in the standard \gls{fp} format, which is automatically extracted by the proposed hardware. The proposed hardware/software architecture is integrated with \gls{tf} Lite. We evaluate the applicability of our approach with a \gls{cnn}-regression model for anomaly localization in a \gls{shm} application based on \glspl{ae}. The embedded hardware/software framework is demonstrated on XC7Z007S, which is the smallest Zynq-7000 \gls{soc}. The proposed implementation achieves a peak power efficiency and run-time acceleration of $5.7$ GFLOPS/s/W and $48.3\times$, respectively.

The outcome of this dissertation aims to contribute to the rise of a sustainable next generation of low-power \gls{fp} neural network processors with \gls{ml} portability as a design philosophy.