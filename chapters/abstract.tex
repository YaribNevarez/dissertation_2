\chapter*{Abstract}
\thispagestyle{empty}
The use of \gls{ai} is entering a new era based on the use of ubiquitous connected devices. The sustainability of this transformation requires the adoption of design techniques that reconcile accurate results with cost-effective system architectures. As such, improving the efficiency of \gls{ai} hardware engines as well as \gls{ml} portability must be considered.

In the emerging era of Industry 4.0, \gls{ml} algorithms yield the power of \gls{ai} to massively ubiquitous \gls{iot} devices. Applications in this field become smarter and more profitable as the availability of big data gets expanded, driving evolution of many aspects in science, industry, and daily life. However, state-of-the-art \gls{ml} algorithms, specially \glspl{snn} and \glspl{cnn}, represent elevated computational and energy cost. Therefore, hardware efficiency is one of the major goals to innovate compute engines as they are the machinery of the future.

Energy, performance, and chip-area are the key design concerns in computer systems. Considering the intrinsic error resilience of \gls{ml} algorithms, paradigms such as approximate computing come to the rescue by offering promising efficiency gains to assist the aforementioned design concerns. Approximation techniques are widely used in \gls{ml} algorithms at the model-structure as well as at the hardware processing level. However, state-of-the-art methods do not sufficiently address accelerator designs for \glspl{ann}, in particular with \gls{fp} computation.

To sustain the continuous expansion of \gls{ml} applications on cost-effective compute devices, approximate computing has the potential to gradually transform from a design alternative to an essential feature. This dissertation focuses on the investigation of design methodologies to exploit the intrinsic error resilience of \gls{ml} algorithms to optimize high-quality \gls{fp} inference in low-power embedded systems.

In the field of \glspl{snn}, this dissertation presents a hardware design methodology for low-power inference of \gls{sbs} neural networks targeting embedded applications. This \gls{ml} algorithm provides noise robustness and reduced complexity compared to conventional \gls{snn} with \gls{lif} mechanism. However, \gls{sbs} networks represent a memory footprint and a computational cost unsuitable for embedded applications. To address this problem, this research exploits the intrinsic error resilience of \gls{sbs} to improve performance and to reduce hardware complexity by approximation. More precisely, it is designed a hardware module to compute vector dot-product based on approximate computing with configurable quality using hybrid custom \gls{fp} and logarithmic number representations. This approach reduces computational run-time, memory footprint, and power dissipation while preserving inference accuracy. To demonstrate this approach, it is presented a design exploration flow with \gls{hls} on a \gls{fpga}. The proposed design accelerates run-time $20.5\times$ and reduces memory footprint $8\times$, with less than $0.5\%$ of accuracy degradation without model retraining on a handwritten digit classification task.

In the field of \glspl{cnn}, this dissertation presents a hardware design methodology for low-power inference targeting \gls{cnn} sensor analytics applications. In this research, it is proposed the \gls{hf6} quantization and its dedicated hardware processor. This design features an optimized \gls{fp} \gls{mac} by reducing the mantissa multiplication to a multiplexer-adder operation. The intrinsic error tolerance of neural networks is exploited to further reduce the hardware design with approximation on the subnormal number computation. To preserve model accuracy, it is presented a \gls{qat} method, which in some cases improves accuracy based on the regularization effect. This concept is demonstrated in a lightweight \gls{tp} implementing a pipelined vector dot-product to accelerate 2D convolution operations. For \gls{ml} portability and backward compatibility, the custom \gls{fp} representation is wrapped in the standard \gls{fp} format. The proposed hardware/software architecture is integrated with \gls{tf} Lite. The applicability of this approach is evaluated with a \gls{cnn}-regression model for anomaly localization in a \gls{shm} application based on \glspl{ae}. The embedded hardware/software framework is demonstrated on XC7Z007S, as the smallest Zynq-7000 \gls{soc}. The proposed implementation achieves a peak power efficiency and run-time acceleration of $5.7$ GFLOPS/s/W and $48.3\times$, respectively.

The outcome of this dissertation aims to contribute to the rise of a sustainable next generation of energy efficient neural network processors with \gls{ml} portability and high-accuracy as design requirements.