\section{Conclusions}
\label{sec:conclusions}
This chapter presents the Hybrid-Float6 quantization and its dedicated hardware accelerator for floating-point \gls{cnn} computation. Feature maps and weights are represented by 32-bit and 6-bit \gls{fp}, respectively. The 6-bit \gls{fp} format is composed of 1-bit sign, 4-bit exponent, and 1-bit mantissa. The 1-bit mantissa enables low-power \gls{mac} implementations by reducing the mantissa multiplication to a multiplexer-adder operation. The intrinsic error tolerance of neural networks is exploited to further reduce the hardware design with approximation. This approach improves latency, hardware area, and energy consumption. To preserve accuracy, a \gls{qat} training method is presented that, based on regularization effects can improve accuracy. A lightweight \gls{tp} implementing a pipelined vector dot-product is presented. For \gls{ml} compatibility/portability, the 6-bit \gls{fp} is wrapped in the standard floating-point format, which is automatically extracted by the proposed hardware. The hardware/software architecture is compatible with TensorFlow Lite. To evaluate the applicability of this approach, it is presented a \gls{cnn}-regression model for anomaly localization in a \gls{shm} application based on acoustic emissions. The embedded hardware/software framework is demonstrated on XC7Z007S as the smallest Zynq-7000 \gls{soc}, suitable for low-power \gls{iot} applications. The proposed architecture achieves a peak power efficiency and acceleration on convolution layers of \unit[5.7]{GFLOPS/s/W} and $48.3\times$, respectively.