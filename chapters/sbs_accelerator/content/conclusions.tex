\section{Conclusions}
\label{sec:conclusions}
In this work, we accelerate SbS neural networks with a dot-product functional unit based on approximate computing that combinesthe advantages of custom floating-point and logarithmic representations. This approach reduces computational latency, memory footprint, and power dissipation while preserving classification accuracy. For output quality monitoring, we applied noise tolerance plots as an intuitive visual measure to provide insights into the accuracy degradation of SbS networks under different approximate processing effects. This plot revels inherent error resilience, hence, the possibilities for approximate processing.


The proposed approach is demonstrated with a design exploration flow on a Xilinx Zynq-7020 with a deployment of \gls{sbs} network for MNIST classification task. This implementation achieves up to $20.5\times$ latency enhancement, $8\times$ weight memory footprint reduction, and $12.35\%$ of energy efficiency improvement over the standard floating-point hardware implementation, this deployment incurs in less than $0.5\%$ of accuracy degradation. Furthermore, with noise amplitude of $50\%$ added on the input images, the \gls{sbs} network presents an accuracy degradation of less than $5\%$. To monitor the inference quality, the resulting noise tolerance plots demonstrate a sufficient \gls{qor} for minimal impact on the overall accuracy of the neural network under the effects of this approximation technique. These results suggest available room for further or more aggressive approximate processing approaches.


In summary, based on the relaxed need for fully accurate or deterministic computation of neural networks, approximate computing techniques allow substantial enhancement in processing efficiency with moderated accuracy degradation.
